{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc5aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fc9e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of dataset in characters: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda5bcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])  # print the first 1000 characters to check the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c82df",
   "metadata": {},
   "source": [
    "Unique characters in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384e3b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size) # integers that range from0 to 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f691b",
   "metadata": {},
   "source": [
    "Tokenizer (from characters to integers (encoder) and back (decoder)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a92c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) } # mapping in a dict each char to its index in the unique char list\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4775e9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # from string to list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # from list of integers to string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\"))) # encode*decode is just null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c04227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # we encode all the text in shakespeare and convert it into a tensor\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476b5aa6",
   "metadata": {},
   "source": [
    "Train validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c34426b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, 111540)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "len(train_data), len(val_data) # 1M chars training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2fcd2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # context length (total of chars going at the same time through the transformer)\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7af2629",
   "metadata": {},
   "source": [
    "We will train it on block_size+1 because we want the next character as target.\n",
    "\n",
    "We will train on all sequences of length 1 up to block_size in the text, not because we don't have the complete sequence, but because we want the model to learn to predict the next character given any context length from 1 up to block_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec0e87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3ca85c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting token positions for each sequence of the 4 ran in parallel: tensor([ 76049, 234249, 934904, 560986])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # sequences we will process in parallel\n",
    "block_size = 8 # context window\n",
    "\n",
    "def get_batch(split, verbose=False): # split is train or val\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    if verbose:\n",
    "        print(f'Starting token positions for each sequence of the {batch_size} ran in parallel: {ix}')\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train', verbose=True)\n",
    "print(xb)\n",
    "print(xb.shape) # input size of the transformer\n",
    "print(yb) # same as xb but with an offset=1\n",
    "print(yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f736b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1] # [row, col] --> [n_batch, input_sequence_len]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31cfb8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f73f0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor([[-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
      "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "        ...,\n",
      "        [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (Batch=4,Time=8,Channel=vocab_size=65)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # pytorch requires (B,C,T) and not (B,T,C) for cross_entropy\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C) # view is like reshape\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # logits are predictions\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx) # self makes it run the forward function\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B,C), so the prob for each token to be the next, and that for each seq\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            # sample from the distribution: it doesn't take the highest prob, but follows the probs we just predicted\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to the running sequence (adds predicted next token to the x tensor)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "            # and goes again to the top with now one token more on the x seq to predict another, until max_new_tokens\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(logits)\n",
    "print(loss) # should be about -ln(1/65)=4.17 , but with some entropy it has it rounds up to 4.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e537d3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "# we choose zero to be the first char; reminder that zero is a \\n character\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), # batch=1, time=1\n",
    "                        max_new_tokens=100)[0].tolist()))\n",
    "# generates a totally random sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291aaff",
   "metadata": {},
   "source": [
    "Now let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ccf6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ad72bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.704006195068359\n",
      "4.658433437347412\n",
      "4.470171928405762\n",
      "4.320702075958252\n",
      "4.252743721008301\n",
      "4.241008758544922\n",
      "4.161406517028809\n",
      "4.044336795806885\n",
      "4.091874122619629\n",
      "3.7458465099334717\n",
      "3.7031264305114746\n",
      "3.7115283012390137\n",
      "3.6330997943878174\n",
      "3.4222123622894287\n",
      "3.429544687271118\n",
      "3.4233598709106445\n",
      "3.3018524646759033\n",
      "3.283510446548462\n",
      "3.188281774520874\n",
      "3.2000553607940674\n",
      "3.1371781826019287\n",
      "3.0028276443481445\n",
      "3.058077812194824\n",
      "2.958632707595825\n",
      "2.9813663959503174\n",
      "2.9196817874908447\n",
      "2.8414011001586914\n",
      "2.8905835151672363\n",
      "2.9735329151153564\n",
      "2.808624029159546\n",
      "2.776794672012329\n",
      "2.748556137084961\n",
      "2.687368392944336\n",
      "2.682086706161499\n",
      "2.688863754272461\n",
      "2.809856653213501\n",
      "2.6931400299072266\n",
      "2.665353298187256\n",
      "2.632939100265503\n",
      "2.75382924079895\n",
      "2.5844571590423584\n",
      "2.630505323410034\n",
      "2.6259851455688477\n",
      "2.5507774353027344\n",
      "2.5834596157073975\n",
      "2.6057393550872803\n",
      "2.6198651790618896\n",
      "2.5730929374694824\n",
      "2.5133121013641357\n",
      "2.608804941177368\n",
      "2.5105180740356445\n",
      "2.574131488800049\n",
      "2.4975621700286865\n",
      "2.525254726409912\n",
      "2.4852945804595947\n",
      "2.5480153560638428\n",
      "2.577272415161133\n",
      "2.602153778076172\n",
      "2.3584303855895996\n",
      "2.448211908340454\n",
      "2.531585931777954\n",
      "2.497114896774292\n",
      "2.446481943130493\n",
      "2.4474494457244873\n",
      "2.530792713165283\n",
      "2.4708240032196045\n",
      "2.5558059215545654\n",
      "2.469895839691162\n",
      "2.605414867401123\n",
      "2.570693254470825\n",
      "2.504757881164551\n",
      "2.444789171218872\n",
      "2.5223405361175537\n",
      "2.4506444931030273\n",
      "2.410496234893799\n",
      "2.400172472000122\n",
      "2.4295127391815186\n",
      "2.424211263656616\n",
      "2.494037389755249\n",
      "2.438490390777588\n",
      "2.4696712493896484\n",
      "2.4945294857025146\n",
      "2.4096620082855225\n",
      "2.6117146015167236\n",
      "2.4675140380859375\n",
      "2.4209394454956055\n",
      "2.4633748531341553\n",
      "2.4416561126708984\n",
      "2.6345114707946777\n",
      "2.3378663063049316\n",
      "2.4838879108428955\n",
      "2.534738779067993\n",
      "2.58534574508667\n",
      "2.3732097148895264\n",
      "2.4463820457458496\n",
      "2.407996892929077\n",
      "2.525810956954956\n",
      "2.521479845046997\n",
      "2.4382166862487793\n",
      "2.4321022033691406\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # instead of the previous 4\n",
    "for i,steps in enumerate(range(10000)):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i%100==0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ffa0bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercc.\n",
      "hathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h hay.JUCle n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZAnde.\n",
      "athave l.\n",
      "KEONH:\n",
      "ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenous s ls, theresseys\n",
      "PlorseelapinghiybHen yof GLUCEN t l-t E:\n",
      "I hisgothers je are!-e!\n",
      "QLYotouciullle'z\n"
     ]
    }
   ],
   "source": [
    "# we use the decoder again for a prediction, expanding the predicting tokens to 500\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317413c5",
   "metadata": {},
   "source": [
    "Until here we have just used the bigram model, so it makes little sense as it has very small context window on previous tokens.\n",
    "\n",
    "Let's now build a proper transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd386959",
   "metadata": {},
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af98214c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba75e9",
   "metadata": {},
   "source": [
    "Tokens should only attend to previous tokens, not future ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ec628",
   "metadata": {},
   "source": [
    "We'll start by calculating the mean of the previous tokens + the current for all channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ec4dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # bow = bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dad582c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7edd282",
   "metadata": {},
   "source": [
    "Okay that's good, but very inefficient. Let's do it with matrix operations using the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15205690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30da7b6",
   "metadata": {},
   "source": [
    "If we look carefully, we can see that the output at time t is just the sum of the first t rows of the input.\n",
    "\n",
    "This is because of the magic that happens by using the lower triangular matrix to only select the tokens up to t."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899898b",
   "metadata": {},
   "source": [
    "Now let's do the mean instead of the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bf1bd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fee196",
   "metadata": {},
   "source": [
    "Let's now do this for the real matrix x of shape (B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53052c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3de2da83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = weights @ x # (T, T) @ (B, T, C) --> (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "torch.allclose(xbow, xbow2, atol=1e-6) # same result, more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bf12a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391bab8",
   "metadata": {},
   "source": [
    "Another final version of the same, now with Softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "684c040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "print(wei)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0fea71",
   "metadata": {},
   "source": [
    "Softmax first exponents all elements, so -inf to 0 and 0 to 1, then normalizes the rows to sum to 1.\n",
    "\n",
    "So first step is:\n",
    "```python\n",
    "[1,0,0,0,0...],\n",
    "[1,1,0,0,0...],\n",
    "[1,1,1,0,0...],\n",
    "[1,1,1,1,0...],\n",
    "```\n",
    "\n",
    "And second step is:\n",
    "```python\n",
    "[1,0,0,0,0...],\n",
    "[0.5,0.5,0,0,0...],\n",
    "[0.33,0.33,0.33,0,0...],\n",
    "[0.25,0.25,0.25,0.25,0...],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b35cdd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae34898",
   "metadata": {},
   "source": [
    "TLDR: you can do weighted aggregation of your past elements by making matrix multiplication of your lower triangular matrix with the elements you want to aggregate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6dbef",
   "metadata": {},
   "source": [
    "## Implementing self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49631bf9",
   "metadata": {},
   "source": [
    "Let's make a fourth version with self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0df12021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # we will use a 32-dimension embedding for each token\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ x\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a54007",
   "metadata": {},
   "source": [
    "Actually, in attention we have queries (what we are looking for) and keys (what we have).\n",
    "\n",
    "The multiplication of all queries with all keys gives us a score of how much each key matches each query.\n",
    "\n",
    "That is equivalent to wei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "561e1e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw connections between keys and queries:\n",
      " tensor([[-0.31, -0.23,  0.10,  0.38, -0.19,  0.35,  0.19, -0.08],\n",
      "        [-0.59, -0.29,  0.02,  0.60, -0.39,  0.18, -0.01,  0.05],\n",
      "        [-0.18, -0.22,  0.01, -0.07, -0.17, -0.25,  0.01, -0.17],\n",
      "        [ 0.14, -0.14, -0.06, -0.15, -0.10, -0.21, -0.23, -0.18],\n",
      "        [-0.22,  0.00, -0.14, -0.23,  0.36,  0.15,  0.07,  0.16],\n",
      "        [-0.06,  0.43, -0.02, -0.18,  0.59, -0.45,  0.25,  0.22],\n",
      "        [ 0.19,  0.35, -0.05, -0.06,  0.11,  0.22, -0.10,  0.14],\n",
      "        [-0.32, -0.07, -0.15,  0.10, -0.14, -0.10,  0.11,  0.11]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Only lower triangle, we cant use the tokens after our token:\n",
      " tensor([[-0.31,  -inf,  -inf,  -inf,  -inf,  -inf,  -inf,  -inf],\n",
      "        [-0.59, -0.29,  -inf,  -inf,  -inf,  -inf,  -inf,  -inf],\n",
      "        [-0.18, -0.22,  0.01,  -inf,  -inf,  -inf,  -inf,  -inf],\n",
      "        [ 0.14, -0.14, -0.06, -0.15,  -inf,  -inf,  -inf,  -inf],\n",
      "        [-0.22,  0.00, -0.14, -0.23,  0.36,  -inf,  -inf,  -inf],\n",
      "        [-0.06,  0.43, -0.02, -0.18,  0.59, -0.45,  -inf,  -inf],\n",
      "        [ 0.19,  0.35, -0.05, -0.06,  0.11,  0.22, -0.10,  -inf],\n",
      "        [-0.32, -0.07, -0.15,  0.10, -0.14, -0.10,  0.11,  0.11]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "After softmax normalization:\n",
      " tensor([[1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
      "        [0.43, 0.57, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
      "        [0.32, 0.30, 0.38, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
      "        [0.30, 0.23, 0.25, 0.23, 0.00, 0.00, 0.00, 0.00],\n",
      "        [0.16, 0.20, 0.18, 0.16, 0.29, 0.00, 0.00, 0.00],\n",
      "        [0.14, 0.23, 0.15, 0.12, 0.27, 0.09, 0.00, 0.00],\n",
      "        [0.16, 0.18, 0.12, 0.12, 0.14, 0.16, 0.12, 0.00],\n",
      "        [0.10, 0.12, 0.11, 0.15, 0.11, 0.12, 0.15, 0.15]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see a single Head perform self-attention\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # we will use a 32-dimension embedding for each token\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1) * C**-0.5 # we want to transpose the last two dimensions: T and head_size --> (B, T, 16) @ (B, 16, T) --> (B, T, T)     C**-0.5 is explained below\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "print('Raw connections between keys and queries:\\n', wei[0])\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T)) --> not anymore\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "print('Only lower triangle, we cant use the tokens after our token:\\n', wei[0])\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print('After softmax normalization:\\n', wei[0])\n",
    "\n",
    "v = value(x)\n",
    "#out = wei @ x\n",
    "out = wei @ v # we use v instead of x, so it'll be 16-dimensional (head_size) instead of 32\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b217e",
   "metadata": {},
   "source": [
    "We have one thing left to explain: the division by sqrt(dk) where dk is the head size.\n",
    "\n",
    "We need to do this to avoid that the variance of the dot products grows too large with the dimension, making the softmax have extremely small gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6aeeef23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.04)\n",
      "tensor(1.07)\n",
      "tensor(17.47)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2,-1)\n",
    "\n",
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4233ebab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.03, 0.00, 0.16, 0.00, 0.80])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d566a89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.90)\n",
      "tensor(1.00)\n",
      "tensor(1.00)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2,-1) * head_size**-0.5\n",
    "\n",
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9db5a581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.19, 0.14, 0.24, 0.14, 0.29])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9db718",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906b0b81",
   "metadata": {},
   "source": [
    "Running multiple attention heads in parallel and concateninating their results.\n",
    "\n",
    "This allows the model to jointly attend to information from different representation subspaces at different positions (this is, attend to different things/patterns at the same time, see the relationships from different perspectives).\n",
    "\n",
    "*Implemented on the multihead-attention.py script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe46645",
   "metadata": {},
   "source": [
    "## Feed-forward MLP layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18abfd",
   "metadata": {},
   "source": [
    "*Implemented on the multihead-attention.py script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa9a45",
   "metadata": {},
   "source": [
    "## Layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a438cb",
   "metadata": {},
   "source": [
    "Same as batch normalization (all batches should have mean 0 and variance 1), but normalizing across the columns instead of the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f9669f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        xmean = x.mean(1, keepdim=True)\n",
    "        xvar = x.var(1, keepdim=True)\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e024678",
   "metadata": {},
   "source": [
    "In fact, PyTorch has a built-in layer for this: nn.LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6d69807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.normalization.LayerNorm"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef43fd63",
   "metadata": {},
   "source": [
    "In the original paper it is applied after each block (attention and feed-forward), but here we will apply it before, as it has later been shown to work better in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e8ecc",
   "metadata": {},
   "source": [
    "Final code block:\n",
    "\n",
    "```python\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" transformer block: communication (multi-head attention) followed by computation (feed-forward) \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embd)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.layernorm1(x))\n",
    "        x = x + self.ffwd(self.layernorm2(x))\n",
    "        return x\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-karpathy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
